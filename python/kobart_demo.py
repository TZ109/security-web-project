# -*- coding: utf-8 -*-
"""KoBART_Demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AM3RL0o-F-qvafzq3xjMWIpIqcKJJ3Mm
"""

import torch
import sys,os,re
from transformers import PreTrainedTokenizerFast
from transformers import BartForConditionalGeneration

def KoBART_summarization(text):
  
    tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')
    model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')
    text = text.replace('\n', ' ')

    raw_input_ids = tokenizer.encode(text)
    input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]

    summary_ids = model.generate(torch.tensor([input_ids]),  num_beams=4,  max_length=512,  eos_token_id=1)
    res = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)
    return res

i = 0
while i<1:
    i=i+1


temp = os.path.isfile(sys.argv[1])
#temp = os.path.isfile('test.txt')
if temp :

    f = open(sys.argv[1],"r", encoding='UTF8')
    #f = open('test.txt',"r", encoding='UTF8')
    line = f.read();
    special = re.compile(r'[^ A-Za-z0-9ㄱ-ㅎ가-힣,.\`\~\"\'\:\;\?\!@#\$%\^\&\*\-\+()\[\]\{\}+]')
    text = special.sub(' ',line)
    result = KoBART_summarization(text)
    print(result);
    f.close();
else : 
    print("파일이 없습니다.")

exit();